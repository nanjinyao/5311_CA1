0|0|### 1. 自动微分（反向模式，动态图计算图）：  
0|1|   通过 `Tensor` + 运算重载 + `backward()`，对模型输出 `F(x)` 求输入/参数的梯度。
0|3|### 2. Integrated Gradients (IG) 归因：  
0|4|   对任意输入（标量/向量/矩阵）计算每个元素对标量输出的贡献度。  
0|5|   - 若模型是用写的 `Tensor` 运算实现 → 用 **AD** 算梯度（快）  
0|6|   - 若模型是普通物理函数或黑盒函数 → 用 **FD 数值微分**算梯度（通用）
0|8|### 3. 哪些运算支持自动微分（AD）？
0|10|在本实现中，**只有当某个运算满足以下两点**时，才支持自动微分（可参与计算图并可反传）：
0|11|1) 该运算的前向会生成新的 `Tensor out`，并记录依赖关系 `out._prev`  
0|12|2) 为该运算实现了反向传播闭包 `out._backward`（定义如何把 `out.grad` 传回父节点）
0|14|**已支持的可微算子（会构图 + 可反传）**
0|15|- 矩阵乘（MatMul）：`*(a::Tensor, b::Tensor)`
0|16|- 加法（带广播反传）：`+(a::Tensor, b::Tensor)`
0|17|- 减法（带广播反传）：`-(a::Tensor, b::Tensor)`
0|18|- 逐元素乘（不重载 `.*`，改用函数）：`mul_elem(a::Tensor, b::Tensor)`
0|19|- 除以标量：`/(a::Tensor, b::Number)`（常用于 mean/平均 loss）
0|20|- ReLU 激活：`relu(t::Tensor)`
0|21|- 索引取标量（2D 单点）：`getindex(x::Tensor, i::Int, j::Int)`（用于选某个 logit 做 IG/求导）
0|22|- 全局求和到标量：`sum(x::Tensor)`
0|24|### 4. 数值微分（FD，非自动微分）
0|26|除自动微分（AD）外，本实现还提供 **数值微分（Finite Difference, FD）** 作为求梯度的备用方案，主要用于：
0|27|- 模型是普通物理函数/黑盒函数（不使用 `Tensor` 构图）
0|28|- 或模型包含尚未实现 `_backward` 的运算
0|30|- 数值微分梯度函数：`grad_fd(model::Function, x::Array{Float64}; eps=1e-5)`
0|31|- 在 IG 中通过 `method=:fd` 调用该函数，在每个插值点上用中心差分近似 \(\nabla_x F\)（通用但计算成本较高）
10|0|### 4. 补充缺失算子 (Power, Inverse, Division, Scalar Mul)
10|1|为了支持物理公式计算，我们需要补充定义的算子：`^` (power), `inv`, `1/x`, 以及标量乘法 `t * n`。12|1|验证我们的自动微分引擎在 $f(x) = \sin(x) \cdot x$ 上的精度。
12|2|比较 **|Custom_AD - Analytic|** 与 **|ForwardDiff - Analytic|** 的误差差距。